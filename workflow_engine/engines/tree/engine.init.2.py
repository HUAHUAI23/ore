"""
æ ‘å½¢å·¥ä½œæµå¼•æ“Žå®žçŽ° - é›†æˆ2025å¹´æœ€æ–°LangChain LLMåŠŸèƒ½

æ”¯æŒå¤šstartèŠ‚ç‚¹ï¼Œåœ¨å›¾æž„å»ºé˜¶æ®µè¿›è¡ŒçŽ¯è·¯æ£€æŸ¥
é›†æˆLangChain 0.3ç‰ˆæœ¬çš„æœ€æ–°LLMè°ƒç”¨æ–¹å¼
"""

import asyncio
import os
from typing import Dict, List, Any, Set, Optional, Union
from collections import defaultdict

# çŽ¯å¢ƒå˜é‡æ”¯æŒ
from dotenv import load_dotenv

# LangChain 2025æœ€æ–°å¯¼å…¥æ–¹å¼
from langchain.chat_models import init_chat_model
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate

from ...base.engine import BaseWorkflowEngine
from ...workflow_types import NodeType
from .types import (
    TreeNode, TreeEdge, Condition, NodeInputData,
    ExecutionSummary,
    TreeCycleError, TreeWorkflowConfig, NodeExecutor, ConditionChecker
)

class TreeWorkflowEngine(BaseWorkflowEngine):
    """æ ‘å½¢å·¥ä½œæµæ‰§è¡Œå¼•æ“Ž - é›†æˆLangChain LLM
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. äº‹ä»¶é©±åŠ¨çš„èŠ‚ç‚¹æ‰§è¡Œ
    2. è‡ªåŠ¨ä¾èµ–æ£€æŸ¥å’Œå¹¶è¡Œæ‰§è¡Œ  
    3. çµæ´»çš„è¾“å…¥å†…å®¹ç»„åˆ
    4. å¤šåˆ†å‰è¾“å‡ºæ”¯æŒ
    5. æ”¯æŒå¤šä¸ªstartèŠ‚ç‚¹
    6. é›†æˆLangChain LLMè°ƒç”¨èƒ½åŠ›ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
    """
    node_executor: NodeExecutor
    condition_checker: ConditionChecker
    nodes: Dict[str, TreeNode]
    edges: List[TreeEdge]
    outgoing_edges: Dict[str, List[TreeEdge]]
    llm_model: Any  # LangChain ChatModelå®žä¾‹
    prompt_templates: Dict[str, ChatPromptTemplate]  # èŠ‚ç‚¹æç¤ºè¯æ¨¡æ¿ç¼“å­˜

    
    def __init__(self, workflow_config: TreeWorkflowConfig) -> None:
        """åˆå§‹åŒ–æ ‘å½¢å·¥ä½œæµå¼•æ“Ž"""
        super().__init__(workflow_config)
        
        # åˆå§‹åŒ–LLM
        self._initialize_llm()
        
        # åˆå§‹åŒ–æç¤ºè¯æ¨¡æ¿ç¼“å­˜
        self.prompt_templates = {}
        
        self.node_executor: NodeExecutor = self._default_node_executor
        self.condition_checker: ConditionChecker = self._default_condition_checker
    
    def _initialize_llm(self) -> None:
        """åˆå§‹åŒ–LangChain LLMæ¨¡åž‹"""
        # åŠ è½½çŽ¯å¢ƒå˜é‡
        load_dotenv()
        
        # ä»ŽçŽ¯å¢ƒå˜é‡èŽ·å–é…ç½®
        llm_provider = os.getenv('LLM_PROVIDER', 'openai')  # é»˜è®¤ä½¿ç”¨OpenAI
        llm_model_name = os.getenv('LLM_MODEL_NAME', 'gpt-3.5-turbo')
        api_key = os.getenv('OPENAI_API_KEY')  # æˆ–å…¶ä»–å¯¹åº”çš„API_KEY
        api_base = os.getenv('API_BASE')  # å¯é€‰çš„APIç«¯ç‚¹
        
        if not api_key:
            raise ValueError(f"æœªæ‰¾åˆ°{llm_provider.upper()}_API_KEYçŽ¯å¢ƒå˜é‡ï¼Œè¯·è®¾ç½®APIå¯†é’¥")
        
        try:
            # ä½¿ç”¨2025å¹´æœ€æ–°çš„init_chat_modelæ–¹å¼
            # æ”¯æŒå¤šç§providerï¼šopenai, anthropic, google_vertexai, ollamaç­‰
            model_identifier = f"{llm_provider}:{llm_model_name}"
            
            # æž„å»ºLLMåˆå§‹åŒ–å‚æ•°
            llm_kwargs = {
                'temperature': float(os.getenv('LLM_TEMPERATURE', '0.7')),
                'max_tokens': int(os.getenv('LLM_MAX_TOKENS', '1000')),
            }
            
            # å¦‚æžœæœ‰è‡ªå®šä¹‰APIç«¯ç‚¹
            if api_base:
                llm_kwargs['base_url'] = api_base
            
            # åˆå§‹åŒ–LLMæ¨¡åž‹
            self.llm_model = init_chat_model(
                model_identifier,
                **llm_kwargs
            )
            
            print(f"âœ… LLMåˆå§‹åŒ–æˆåŠŸ: {model_identifier}")
            print(f"   Temperature: {llm_kwargs['temperature']}")
            print(f"   Max Tokens: {llm_kwargs['max_tokens']}")
            
        except Exception as e:
            print(f"âŒ LLMåˆå§‹åŒ–å¤±è´¥: {e}")
            print("ðŸ”§ è¯·æ£€æŸ¥ä»¥ä¸‹çŽ¯å¢ƒå˜é‡é…ç½®:")
            print("   - OPENAI_API_KEY (æˆ–å¯¹åº”providerçš„API_KEY)")
            print("   - LLM_PROVIDER (å¯é€‰ï¼Œé»˜è®¤openai)")
            print("   - LLM_MODEL_NAME (å¯é€‰ï¼Œé»˜è®¤gpt-3.5-turbo)")
            print("   - API_BASE (å¯é€‰ï¼Œè‡ªå®šä¹‰APIç«¯ç‚¹)")
            raise
    
    def _create_prompt_template(self, node: TreeNode, input_data: NodeInputData) -> ChatPromptTemplate:
        """åˆ›å»ºæˆ–èŽ·å–èŠ‚ç‚¹çš„æç¤ºè¯æ¨¡æ¿
        
        Args:
            node: èŠ‚ç‚¹ä¿¡æ¯
            input_data: è¾“å…¥æ•°æ®
            
        Returns:
            ChatPromptTemplateå®žä¾‹
        """
        # ä½¿ç”¨èŠ‚ç‚¹IDä½œä¸ºç¼“å­˜é”®
        cache_key = f"{node.id}_{hash(str(input_data.prompt))}"
        
        if cache_key in self.prompt_templates:
            return self.prompt_templates[cache_key]
        
        # æž„å»ºæ¶ˆæ¯æ¨¡æ¿åˆ—è¡¨
        messages = []
        
        # 1. ç³»ç»Ÿæ¶ˆæ¯ï¼šè®¾ç½®AIè§’è‰²å’Œä»»åŠ¡æè¿°
        system_content_parts = []
        
        # åŸºç¡€è§’è‰²è®¾å®š
        system_content_parts.append("ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIå·¥ä½œæµå¤„ç†åŠ©æ‰‹ã€‚")
        
        # èŠ‚ç‚¹ç‰¹å®šæè¿°
        if node.description:
            system_content_parts.append(f"å½“å‰ä»»åŠ¡ï¼š{node.description}")
        
        # èŠ‚ç‚¹ç±»åž‹ç‰¹å®šæŒ‡ä»¤
        if node.node_type == NodeType.START:
            system_content_parts.append("è¿™æ˜¯å·¥ä½œæµçš„èµ·å§‹èŠ‚ç‚¹ï¼Œè¯·åˆå§‹åŒ–ç›¸å…³ä¿¡æ¯ã€‚")
        elif node.node_type == NodeType.LEAF:
            system_content_parts.append("è¿™æ˜¯å·¥ä½œæµçš„å¶å­èŠ‚ç‚¹ï¼Œè¯·äº§ç”Ÿæœ€ç»ˆç»“æžœã€‚")
        else:
            system_content_parts.append("è¯·å¤„ç†è¾“å…¥ä¿¡æ¯å¹¶ç”Ÿæˆé«˜è´¨é‡çš„è¾“å‡ºã€‚")
        
        # è¾“å‡ºæ ¼å¼è¦æ±‚
        system_content_parts.append("è¯·æä¾›æ¸…æ™°ã€å‡†ç¡®ä¸”æœ‰ç”¨çš„å›žå¤ã€‚")
        
        system_content = "\n".join(system_content_parts)
        messages.append(SystemMessage(content=system_content))
        
        # 2. äººç±»æ¶ˆæ¯ï¼šç»„åˆä»»åŠ¡æç¤ºå’Œå‰åºè¾“å‡º
        user_content_parts = []
        
        # ä»»åŠ¡æç¤ºè¯ï¼ˆå¦‚æžœæœ‰ï¼‰
        if input_data.prompt:
            user_content_parts.append(f"**ä»»åŠ¡æŒ‡ä»¤ï¼š**\n{input_data.prompt}")
        
        # å‰åºèŠ‚ç‚¹è¾“å‡ºï¼ˆå¦‚æžœæœ‰ï¼‰
        if input_data.previous_output:
            user_content_parts.append(f"**å‰åºç»“æžœï¼š**\n{input_data.previous_output}")
        
        # å¦‚æžœæ²¡æœ‰å…·ä½“å†…å®¹ï¼Œä½¿ç”¨èŠ‚ç‚¹åç§°ä½œä¸ºé»˜è®¤ä»»åŠ¡
        if not user_content_parts:
            user_content_parts.append(f"**å¤„ç†ä»»åŠ¡ï¼š**\n{node.name}")
        
        # æ·»åŠ å¤„ç†æŒ‡å¼•
        user_content_parts.append("**è¯·æ±‚ï¼š**\nè¯·åŸºäºŽä»¥ä¸Šä¿¡æ¯è¿›è¡Œå¤„ç†å¹¶æä¾›ç»“æžœã€‚")
        
        user_content = "\n\n".join(user_content_parts)
        messages.append(HumanMessage(content=user_content))
        
        # 3. åˆ›å»ºChatPromptTemplate
        template = ChatPromptTemplate.from_messages(messages)
        
        # ç¼“å­˜æ¨¡æ¿
        self.prompt_templates[cache_key] = template
        
        return template
    
    def _initialize_from_config(self, config: TreeWorkflowConfig) -> None:
        """ä»Žé…ç½®åˆå§‹åŒ–æ ‘å½¢å¼•æ“Ž"""
        # è§£æžèŠ‚ç‚¹å’Œè¾¹
        self.nodes: Dict[str, TreeNode] = {
            node_id: TreeNode.from_dict(node_data) 
            for node_id, node_data in config['nodes'].items()
        }
        self.edges: List[TreeEdge] = [
            TreeEdge.from_dict(edge_data) 
            for edge_data in config['edges']
        ]
        
        # æž„å»ºæ‰§è¡Œå›¾å¹¶æ£€æŸ¥çŽ¯è·¯
        self.outgoing_edges = self._build_execution_graph()
    
    def _build_execution_graph(self) -> Dict[str, List[TreeEdge]]:
        """æž„å»ºæ‰§è¡Œå›¾ï¼ˆé‚»æŽ¥è¡¨ï¼‰å¹¶æ£€æŸ¥çŽ¯è·¯"""
        graph: Dict[str, List[TreeEdge]] = defaultdict(list)
        for edge in self.edges:
            graph[edge.from_node].append(edge)
        
        # åœ¨æž„å»ºæ—¶æ£€æŸ¥çŽ¯è·¯
        if self._has_cycle(graph):
            raise TreeCycleError("æ£€æµ‹åˆ°çŽ¯è·¯ï¼Œæ ‘å½¢å·¥ä½œæµä¸å…è®¸çŽ¯è·¯")
        
        return dict(graph)
    
    def _has_cycle(self, graph: Dict[str, List[TreeEdge]]) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰çŽ¯è·¯ï¼ˆDFSæ£€æµ‹ï¼‰"""
        # çŠ¶æ€ï¼š0-æœªè®¿é—®ï¼Œ1-æ­£åœ¨è®¿é—®ï¼Œ2-å·²è®¿é—®
        state = {node_id: 0 for node_id in self.nodes.keys()}
        
        def dfs(node_id: str) -> bool:
            if state[node_id] == 1:  # æ­£åœ¨è®¿é—®ä¸­ï¼Œå‘çŽ°å›žè¾¹
                return True
            if state[node_id] == 2:  # å·²è®¿é—®è¿‡
                return False
            
            state[node_id] = 1  # æ ‡è®°ä¸ºæ­£åœ¨è®¿é—®
            
            # è®¿é—®æ‰€æœ‰é‚»å±…
            for edge in graph.get(node_id, []):
                if dfs(edge.to_node):
                    return True
            
            state[node_id] = 2  # æ ‡è®°ä¸ºå·²è®¿é—®
            return False
        
        # å¯¹æ‰€æœ‰æœªè®¿é—®çš„èŠ‚ç‚¹è¿›è¡ŒDFS
        for node_id in self.nodes.keys():
            if state[node_id] == 0:
                if dfs(node_id):
                    return True
        
        return False
    
    async def execute_workflow(self) -> ExecutionSummary:
        """æ‰§è¡Œæ ‘å½¢å·¥ä½œæµ"""
        print(f"ðŸš€ æ‰§è¡Œå·¥ä½œæµ: {self.workflow_name} ({self.workflow_id})")
        
        # æ‰¾åˆ°æ‰€æœ‰èµ·å§‹èŠ‚ç‚¹
        start_nodes = [node_id for node_id, node in self.nodes.items() 
                      if node.node_type == NodeType.START]
        
        if not start_nodes:
            raise ValueError("æœªæ‰¾åˆ°èµ·å§‹èŠ‚ç‚¹")
        
        print(f"ðŸŽ¯ èµ·å§‹èŠ‚ç‚¹: {[self.nodes[nid].name for nid in start_nodes]}")
        
        # å¯åŠ¨æ‰€æœ‰èµ·å§‹èŠ‚ç‚¹
        for start_node_id in start_nodes:
            result = await self.node_executor(start_node_id, NodeInputData())
            self._mark_node_completed(start_node_id, result)
            await self._try_trigger_next_nodes(start_node_id)
        
        # äº‹ä»¶é©±åŠ¨å¾ªçŽ¯
        await self._run_execution_loop()
        
        print("âœ… å·¥ä½œæµæ‰§è¡Œå®Œæˆï¼")
        
        # ç”Ÿæˆæ‰§è¡Œæ‘˜è¦
        execution_summary = ExecutionSummary(
            workflow_id=self.workflow_id,
            workflow_name=self.workflow_name,
            completed_count=len(self.completed_nodes),
            total_count=len(self.nodes),
            results=self.node_results
        )
        
        return execution_summary

    async def _run_execution_loop(self) -> None:
        """è¿è¡Œäº‹ä»¶é©±åŠ¨çš„æ‰§è¡Œå¾ªçŽ¯"""
        while self.running_tasks:
            completed_task_ids = []
            for node_id, task in self.running_tasks.items():
                if task.done():
                    completed_task_ids.append(node_id)
                    
                    try:
                        result = await task
                        self._mark_node_completed(node_id, result)
                    except Exception as e:
                        print(f"âŒ èŠ‚ç‚¹ {self.nodes[node_id].name} æ‰§è¡Œå¤±è´¥: {e}")
                        self._mark_node_failed(node_id, e)
            
            # æ¸…ç†å·²å®Œæˆçš„ä»»åŠ¡å¹¶è§¦å‘åŽç»­èŠ‚ç‚¹
            for node_id in completed_task_ids:
                del self.running_tasks[node_id]
                await self._try_trigger_next_nodes(node_id)
            
            # çŸ­æš‚ä¼‘çœ é¿å…å¿™ç­‰å¾…
            if self.running_tasks:
                await asyncio.sleep(0.1)
    
    async def _try_trigger_next_nodes(self, completed_node_id: str) -> None:
        """å°è¯•è§¦å‘åŽç»­èŠ‚ç‚¹"""
        outgoing_edges = self.outgoing_edges.get(completed_node_id, [])
        
        for edge in outgoing_edges:
            target_node_id = edge.to_node
            
            # å…ˆæ£€æŸ¥æ˜¯å¦æ‰€æœ‰å‰é©±èŠ‚ç‚¹éƒ½å·²å®Œæˆ
            if not self._check_all_prerequisites_completed(target_node_id):
                print(f"â³ èŠ‚ç‚¹ {self.nodes[target_node_id].name} ç­‰å¾…å‰é©±èŠ‚ç‚¹å®Œæˆ")
                continue

            # ç„¶åŽæ£€æŸ¥æ¡ä»¶æ˜¯å¦æ»¡è¶³
            if edge.condition is not None:
                if not self.condition_checker(edge.condition, self.node_results[completed_node_id]):
                    node_name = self.nodes[target_node_id].name
                    print(f"ðŸš« æ¡ä»¶ä¸æ»¡è¶³ï¼Œè·³è¿‡èŠ‚ç‚¹: {node_name}")
                    continue
                
            # é¿å…é‡å¤æ‰§è¡Œ
            if target_node_id in self.running_tasks or target_node_id in self.completed_nodes:
                continue
            
            # å‡†å¤‡è¾“å…¥æ•°æ®å¹¶å¯åŠ¨ä»»åŠ¡
            input_data = self._prepare_node_input(target_node_id, edge)
            task = asyncio.create_task(self.node_executor(target_node_id, input_data))
            self.running_tasks[target_node_id] = task
            
            node = self.nodes[target_node_id]
            print(f"ðŸ”„ å¯åŠ¨èŠ‚ç‚¹: {node.name} ({node.description})")
    
    def _check_all_prerequisites_completed(self, node_id: str) -> bool:
        """æ£€æŸ¥èŠ‚ç‚¹çš„æ‰€æœ‰å‰é©±æ˜¯å¦éƒ½å·²å®Œæˆ"""
        prerequisites = set()
        
        # æ”¶é›†æ‰€æœ‰æŒ‡å‘è¯¥èŠ‚ç‚¹çš„å‰é©±
        for edge in self.edges:
            if edge.to_node == node_id:
                prerequisites.add(edge.from_node)
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å‰é©±éƒ½å·²å®Œæˆ
        missing_prereqs = prerequisites - self.completed_nodes
        if missing_prereqs:
            missing_names = [self.nodes[pid].name for pid in missing_prereqs]
            print(f"   ç­‰å¾…å‰é©±: {', '.join(missing_names)}")
            return False
        
        return True
    
    def _prepare_node_input(self, node_id: str, edge: TreeEdge) -> NodeInputData:
        """æ ¹æ®è¾¹é…ç½®å‡†å¤‡èŠ‚ç‚¹è¾“å…¥"""
        node = self.nodes[node_id]
        input_config = edge.input_config
        
        # 1. ä»»åŠ¡æç¤ºè¯
        prompt = node.prompt if input_config.include_prompt else None
        
        # 2. æ”¶é›†æ‰€æœ‰ç›´æŽ¥å‰é©±çš„è¾“å‡º - å…³é”®ä¿®æ­£
        previous_output = None
        if input_config.include_previous_output:
            # âœ… æ–°é€»è¾‘ï¼šæ ¹æ®node_idæ”¶é›†æ‰€æœ‰å‰é©±
            all_predecessors = self._get_all_completed_predecessors(node_id)
            if all_predecessors:
                previous_output = self._combine_predecessor_outputs(all_predecessors)
        
        return NodeInputData(prompt=prompt, previous_output=previous_output)

    def _get_all_completed_predecessors(self, node_id: str) -> List[str]:
        """èŽ·å–æŒ‡å®šèŠ‚ç‚¹çš„æ‰€æœ‰å·²å®Œæˆçš„ç›´æŽ¥å‰é©±èŠ‚ç‚¹
        
        Args:
            node_id: ç›®æ ‡èŠ‚ç‚¹ID
            
        Returns:
            å·²å®Œæˆçš„å‰é©±èŠ‚ç‚¹IDåˆ—è¡¨
        """
        predecessors = []
        
        # éåŽ†æ‰€æœ‰è¾¹ï¼Œæ‰¾åˆ°æŒ‡å‘ç›®æ ‡èŠ‚ç‚¹çš„è¾¹
        for edge in self.edges:
            if (edge.to_node == node_id and 
                edge.from_node in self.completed_nodes):
                predecessors.append(edge.from_node)
        
        # åŽ»é‡ä¿æŒé¡ºåºï¼ˆé¿å…é‡å¤è¾¹çš„æƒ…å†µï¼‰
        seen = set()
        unique_predecessors = []
        for pred_id in predecessors:
            if pred_id not in seen:
                seen.add(pred_id)
                unique_predecessors.append(pred_id)
        
        return unique_predecessors
    
    def _combine_predecessor_outputs(self, predecessor_ids: List[str]) -> str:
        """ç»„åˆå¤šä¸ªå‰é©±èŠ‚ç‚¹çš„è¾“å‡ºç»“æžœ
        
        Args:
            predecessor_ids: å‰é©±èŠ‚ç‚¹IDåˆ—è¡¨
            
        Returns:
            ç»„åˆåŽçš„è¾“å‡ºå­—ç¬¦ä¸²
        """
        if not predecessor_ids:
            return ""
        
        # å•ä¸ªå‰é©±ï¼Œç›´æŽ¥è¿”å›žç»“æžœ
        if len(predecessor_ids) == 1:
            return str(self.node_results[predecessor_ids[0]])
        
        # å¤šä¸ªå‰é©±ï¼Œç”¨æ ‡è¯†ç¬¦ç»„åˆ
        combined_parts = []
        for pred_id in predecessor_ids:
            pred_name = self.nodes[pred_id].name
            pred_result = self.node_results[pred_id]
            combined_parts.append(f"[{pred_name}]: {pred_result}")
        
        return " | ".join(combined_parts)
    
    def _default_condition_checker(self, condition: Optional[Condition], node_output: Any) -> bool:
        """é»˜è®¤æ¡ä»¶æ£€æŸ¥å™¨"""
        if condition is None:
            return True
        return condition.check(node_output)
    
    async def _default_node_executor(self, node_id: str, input_data: NodeInputData) -> Any:
        """ä¼˜åŒ–ç‰ˆèŠ‚ç‚¹æ‰§è¡Œå™¨ - é›†æˆChatPromptTemplateå’Œæœ€æ–°LangChainè¯­æ³•"""
        node = self.nodes[node_id]
        
        # èµ·å§‹èŠ‚ç‚¹ç›´æŽ¥è¿”å›ž
        if node.node_type == NodeType.START:
            return f"START_{self.workflow_id}"
        
        try:
            # åˆ›å»ºæç¤ºè¯æ¨¡æ¿
            prompt_template = self._create_prompt_template(node, input_data)
            
            print(f"ðŸ¤– è°ƒç”¨LLMå¤„ç†èŠ‚ç‚¹: {node.name}")
            print(f"   èŠ‚ç‚¹ç±»åž‹: {node.node_type.value}")
            
            # æ ¼å¼åŒ–æç¤ºè¯æ¨¡æ¿
            formatted_prompt = prompt_template.invoke({})
            
            # æ˜¾ç¤ºè¾“å…¥ä¿¡æ¯
            total_input_length = sum(len(msg.content) for msg in formatted_prompt.to_messages())
            print(f"   è¾“å…¥æ¶ˆæ¯æ•°: {len(formatted_prompt.to_messages())}")
            print(f"   æ€»è¾“å…¥é•¿åº¦: {total_input_length} å­—ç¬¦")
            
            # ä½¿ç”¨å¼‚æ­¥è°ƒç”¨LLM - 2025å¹´æœ€æ–°è¯­æ³•
            try:
                # ä¼˜å…ˆä½¿ç”¨ainvokeè¿›è¡Œå¼‚æ­¥è°ƒç”¨
                response = await self.llm_model.ainvoke(formatted_prompt.to_messages())
                
                # æå–å“åº”å†…å®¹
                if hasattr(response, 'content'):
                    result = response.content
                else:
                    result = str(response)
                
                print(f"âœ… LLMå¼‚æ­¥å“åº”å®Œæˆï¼Œè¾“å‡ºé•¿åº¦: {len(result)} å­—ç¬¦")
                
                # å¯¹ç‰¹å®šèŠ‚ç‚¹ç±»åž‹æ·»åŠ é¢å¤–å¤„ç†
                if node.node_type == NodeType.LEAF:
                    result = f"[FINAL RESULT] {result}"
                
                return result
                
            except AttributeError:
                # å¦‚æžœæ¨¡åž‹ä¸æ”¯æŒainvokeï¼Œå›žé€€åˆ°åŒæ­¥è°ƒç”¨
                print("âš ï¸  æ¨¡åž‹ä¸æ”¯æŒå¼‚æ­¥è°ƒç”¨ï¼Œå›žé€€åˆ°åŒæ­¥æ¨¡å¼")
                response = await asyncio.to_thread(
                    self.llm_model.invoke, 
                    formatted_prompt.to_messages()
                )
                
                if hasattr(response, 'content'):
                    result = response.content
                else:
                    result = str(response)
                
                print(f"âœ… LLMåŒæ­¥å“åº”å®Œæˆï¼Œè¾“å‡ºé•¿åº¦: {len(result)} å­—ç¬¦")
                return result
            
        except Exception as e:
            print(f"âŒ LLMè°ƒç”¨å¤±è´¥: {e}")
            print(f"   é”™è¯¯ç±»åž‹: {type(e).__name__}")
            
            # é™çº§åˆ°ç®€å•æ–‡æœ¬å¤„ç†
            return await self._fallback_text_processing(node, input_data)
    
    async def _fallback_text_processing(self, node: TreeNode, input_data: NodeInputData) -> str:
        """LLMè°ƒç”¨å¤±è´¥æ—¶çš„é™çº§å¤„ç† - å¢žå¼ºç‰ˆ"""
        print(f"ðŸ”„ é™çº§åˆ°æ–‡æœ¬å¤„ç†æ¨¡å¼: {node.name}")
        
        # ç»„è£…å¤„ç†è¾“å…¥
        input_parts = []
        
        if input_data.prompt is not None:
            input_parts.append(f"PROMPT: {input_data.prompt}")
        
        if input_data.previous_output is not None:
            input_parts.append(f"PREV: {input_data.previous_output}")
        
        # æ ¹æ®èŠ‚ç‚¹ç±»åž‹è¿›è¡Œä¸åŒçš„å¤„ç†
        if node.node_type == NodeType.START:
            result = f"START[{node.name}]: å·¥ä½œæµå·²å¯åŠ¨"
        elif node.node_type == NodeType.LEAF:
            full_input = " | ".join(input_parts)
            result = f"FINAL[{node.name}]: {full_input[:200]}..."
        else:
            full_input = " | ".join(input_parts)
            result = f"PROCESSED[{node.name}]: {full_input[:150]}..."
        
        # æ¨¡æ‹Ÿå¤„ç†å»¶è¿Ÿ
        await asyncio.sleep(0.2)
        
        return result
    
    def _mark_node_completed(self, node_id: str, result: Any) -> None:
        """é‡å†™ä»¥æä¾›æ ‘å½¢ç‰¹æœ‰çš„æ—¥å¿—"""
        super()._mark_node_completed(node_id, result)
        node = self.nodes[node_id]
        
        if node.node_type == NodeType.LEAF:
            print(f"ðŸŽ¯ å¶å­èŠ‚ç‚¹å®Œæˆ: {node.name}")
        elif node.node_type == NodeType.START:
            print(f"âœ“ èµ·å§‹èŠ‚ç‚¹å®Œæˆ: {node.name}")
        else:
            print(f"âœ“ èŠ‚ç‚¹å®Œæˆ: {node.name}") 
    
    def _mark_node_failed(self, node_id: str, error: Exception) -> None:
        """é‡å†™ä»¥æä¾›æ ‘å½¢ç‰¹æœ‰çš„æ—¥å¿—"""
        super()._mark_node_failed(node_id, error)
        node = self.nodes[node_id]
        print(f"âŒ èŠ‚ç‚¹å¤±è´¥: {node.name}")
        print(f"   é”™è¯¯: {error}")
        
    def get_prompt_template_info(self, node_id: str) -> Dict[str, Any]:
        """èŽ·å–èŠ‚ç‚¹çš„æç¤ºè¯æ¨¡æ¿ä¿¡æ¯ï¼ˆè°ƒè¯•ç”¨ï¼‰
        
        Args:
            node_id: èŠ‚ç‚¹ID
            
        Returns:
            æ¨¡æ¿ä¿¡æ¯å­—å…¸
        """
        if node_id not in self.nodes:
            return {"error": "èŠ‚ç‚¹ä¸å­˜åœ¨"}
        
        node = self.nodes[node_id]
        dummy_input = NodeInputData(
            prompt=node.prompt,
            previous_output="[ç¤ºä¾‹å‰åºè¾“å‡º]"
        )
        
        template = self._create_prompt_template(node, dummy_input)
        formatted = template.invoke({})
        
        return {
            "node_name": node.name,
            "node_type": node.node_type.value,
            "template_messages": len(formatted.to_messages()),
            "messages": [
                {
                    "role": msg.__class__.__name__,
                    "content": str(msg.content)[:200] + "..." if len(str(msg.content)) > 200 else str(msg.content)
                }
                for msg in formatted.to_messages()
            ]
        }